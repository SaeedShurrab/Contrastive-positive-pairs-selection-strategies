{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeCosineSimilarity(nn.Module):\n",
    "    def __init__(self,\n",
    "                 mode: str = 'simplified'\n",
    "                ) -> None:\n",
    "        super(NegativeCosineSimilarity,self).__init__()\n",
    "        \n",
    "        self.mode = mode\n",
    "        assert self.mode in ['simplified', 'original'], \\\n",
    "        'loss mode must be either (simplified) or (original)'\n",
    "        \n",
    "        \n",
    "    def _forward1(self,\n",
    "                  p: Tensor,\n",
    "                  z: Tensor,\n",
    "                 ) -> Tensor:\n",
    "        z = z.detach()\n",
    "        p = F.normalize(p, dim=1)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        loss = -(p*z).sum(dim=1).mean()\n",
    "        return loss\n",
    "        \n",
    "    def _forward2(self,\n",
    "                  p: Tensor,\n",
    "                  z: Tensor,\n",
    "                 ) -> Tensor:\n",
    "        z = z.detach\n",
    "        loss = - F.cosine_similarity(p, z, dim=-1).mean()\n",
    "        return loss\n",
    "        \n",
    "    def forward(self,\n",
    "                  p1: Tensor,\n",
    "                  p2: Tensor,\n",
    "                  z1: Tensor,\n",
    "                  z2: Tensor,\n",
    "                 ) -> Tensor:\n",
    "        \n",
    "        if self.mode == 'original':\n",
    "            loss1 = self._forward1(p1,z2)\n",
    "            loss2 = self._forward1(p2,z1)\n",
    "            loss = loss1/2 +loss2/2\n",
    "            return loss\n",
    "        \n",
    "        elif self.mode == 'simplified':\n",
    "            loss1 = self._forward1(p1,z2)\n",
    "            loss2 = self._forward1(p2,z1)\n",
    "            loss = loss1/2 +loss2/2\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2974)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NegativeCosineSimilarity()\n",
    "z1 = torch.randn((1, 10))\n",
    "z2 = torch.rand_like(z1)\n",
    "p1 = torch.randn((1, 10))\n",
    "p2 = torch.rand_like(p1)\n",
    "criterion = NegativeCosineSimilarity()\n",
    "criterion(p1,p2,z1,z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2974)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = NegativeCosineSimilarity('original')\n",
    "criterion.forward(p1,p2,z1,z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int = 2048,\n",
    "                 output_dim: int = 2048,\n",
    "                ) -> None:\n",
    "        super(ProjectionMLP,self).__init__()\n",
    "        \n",
    "        \n",
    "\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_features=input_dim, out_features= hidden_dim, bias=False ),\n",
    "                                    nn.BatchNorm1d(hidden_dim),\n",
    "                                    nn.ReLU(inplace=True)\n",
    "                                   )\n",
    "\n",
    "        self.layer2 = nn.Sequential(nn.Linear(in_features=hidden_dim, out_features=hidden_dim, bias=False),\n",
    "                                    nn.BatchNorm1d(hidden_dim),\n",
    "                                    nn.ReLU(inplace=True)\n",
    "                                   )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=False),\n",
    "                                    nn.BatchNorm1d(hidden_dim)\n",
    "                                   )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9948, -0.9908, -0.9999,  ..., -0.9918, -0.9999, -1.0000],\n",
       "        [ 0.9948,  0.9908,  0.9999,  ...,  0.9918,  0.9999,  1.0000]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,2048)\n",
    "model = ProjectionMLP(2048)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int = 2048,\n",
    "                 hidden_dim: int = 512,\n",
    "                 output_dim: int = 2048,\n",
    "                ) -> None:\n",
    "        super(PredictionMLP,self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_features=input_dim, out_features=hidden_dim, bias= False),\n",
    "                                    nn.BatchNorm1d(hidden_dim),\n",
    "                                    nn.ReLU(inplace=True)\n",
    "                                   )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(nn.Linear(in_features=hidden_dim, out_features=output_dim))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3141, -0.2764,  0.7324,  ...,  0.1383, -0.5016, -0.1909],\n",
       "        [ 0.1296, -0.7277, -0.4366,  ..., -0.1094,  0.6023, -0.2056]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = PredictionMLP()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodProject(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 hidden_dim: int = 2048,\n",
    "                 output_dim: int = 2048\n",
    "                 ) -> None:\n",
    "        super(EncodProject, self).__init__()\n",
    "                \n",
    "        self.encoder = nn.Sequential(*list(model.children())[:-1])\n",
    "        \n",
    "        self.projector = ProjectionMLP(input_dim=nn.Sequential(*list(model.children()))[-1].in_features,\n",
    "                                       hidden_dim=hidden_dim,\n",
    "                                       output_dim=output_dim\n",
    "                                       )\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.projector(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 projector_hidden_dim: int = 2048,\n",
    "                 projector_output_dim: int = 2048,\n",
    "                 predictor_hidden_dim: int = 512,\n",
    "                 predictor_output_dim: int = 2048\n",
    "                ) -> None: \n",
    "        super(SimSiam, self).__init__()\n",
    "        \n",
    "        self.encode_project = EncodProject(model, \n",
    "                                           hidden_dim= projector_hidden_dim,\n",
    "                                           output_dim= projector_hidden_dim\n",
    "                                          )\n",
    "        self.predictor = PredictionMLP(input_dim=projector_output_dim,\n",
    "                                       hidden_dim=predictor_hidden_dim,\n",
    "                                       output_dim=predictor_output_dim)\n",
    "        \n",
    "    def forward(self, \n",
    "                x1: Tensor,\n",
    "                x2: Tensor\n",
    "               ) -> Tuple[Tensor]:\n",
    "        \n",
    "        f, h = self.encode_project, self.predictor\n",
    "        z1, z2 = f(x1), f(x2)\n",
    "        p1, p2 = h(z1), h(z2)\n",
    "        \n",
    "        \n",
    "        return {'p1': p1,\n",
    "                'p2' : p2,\n",
    "                'z1' : z1,\n",
    "                'z2' : z2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(-0.0045, grad_fn=<AddBackward0>)\n",
      "2 tensor(-0.0239, grad_fn=<AddBackward0>)\n",
      "3 tensor(-0.0532, grad_fn=<AddBackward0>)\n",
      "4 tensor(-0.0805, grad_fn=<AddBackward0>)\n",
      "5 tensor(-0.1194, grad_fn=<AddBackward0>)\n",
      "6 tensor(-0.1787, grad_fn=<AddBackward0>)\n",
      "7 tensor(-0.2419, grad_fn=<AddBackward0>)\n",
      "8 tensor(-0.3658, grad_fn=<AddBackward0>)\n",
      "9 tensor(-0.4321, grad_fn=<AddBackward0>)\n",
      "10 tensor(-0.5377, grad_fn=<AddBackward0>)\n",
      "11 tensor(-0.5980, grad_fn=<AddBackward0>)\n",
      "12 tensor(-0.6544, grad_fn=<AddBackward0>)\n",
      "13 tensor(-0.7177, grad_fn=<AddBackward0>)\n",
      "14 tensor(-0.7648, grad_fn=<AddBackward0>)\n",
      "15 tensor(-0.7951, grad_fn=<AddBackward0>)\n",
      "16 tensor(-0.8146, grad_fn=<AddBackward0>)\n",
      "17 tensor(-0.8446, grad_fn=<AddBackward0>)\n",
      "18 tensor(-0.8590, grad_fn=<AddBackward0>)\n",
      "19 tensor(-0.8557, grad_fn=<AddBackward0>)\n",
      "20 tensor(-0.8766, grad_fn=<AddBackward0>)\n",
      "21 tensor(-0.8814, grad_fn=<AddBackward0>)\n",
      "22 tensor(-0.8823, grad_fn=<AddBackward0>)\n",
      "23 tensor(-0.8889, grad_fn=<AddBackward0>)\n",
      "24 tensor(-0.8808, grad_fn=<AddBackward0>)\n",
      "25 tensor(-0.8911, grad_fn=<AddBackward0>)\n",
      "26 tensor(-0.8934, grad_fn=<AddBackward0>)\n",
      "27 tensor(-0.8771, grad_fn=<AddBackward0>)\n",
      "28 tensor(-0.8895, grad_fn=<AddBackward0>)\n",
      "29 tensor(-0.8911, grad_fn=<AddBackward0>)\n",
      "30 tensor(-0.8816, grad_fn=<AddBackward0>)\n",
      "31 tensor(-0.8842, grad_fn=<AddBackward0>)\n",
      "32 tensor(-0.8832, grad_fn=<AddBackward0>)\n",
      "33 tensor(-0.8757, grad_fn=<AddBackward0>)\n",
      "34 tensor(-0.8838, grad_fn=<AddBackward0>)\n",
      "35 tensor(-0.8746, grad_fn=<AddBackward0>)\n",
      "36 tensor(-0.8794, grad_fn=<AddBackward0>)\n",
      "37 tensor(-0.8775, grad_fn=<AddBackward0>)\n",
      "38 tensor(-0.8794, grad_fn=<AddBackward0>)\n",
      "39 tensor(-0.8600, grad_fn=<AddBackward0>)\n",
      "40 tensor(-0.8782, grad_fn=<AddBackward0>)\n",
      "41 tensor(-0.8866, grad_fn=<AddBackward0>)\n",
      "42 tensor(-0.8989, grad_fn=<AddBackward0>)\n",
      "43 tensor(-0.8988, grad_fn=<AddBackward0>)\n",
      "44 tensor(-0.8937, grad_fn=<AddBackward0>)\n",
      "45 tensor(-0.9030, grad_fn=<AddBackward0>)\n",
      "46 tensor(-0.9053, grad_fn=<AddBackward0>)\n",
      "47 tensor(-0.9033, grad_fn=<AddBackward0>)\n",
      "48 tensor(-0.9064, grad_fn=<AddBackward0>)\n",
      "49 tensor(-0.9132, grad_fn=<AddBackward0>)\n",
      "50 tensor(-0.9201, grad_fn=<AddBackward0>)\n",
      "51 tensor(-0.9228, grad_fn=<AddBackward0>)\n",
      "52 tensor(-0.9229, grad_fn=<AddBackward0>)\n",
      "53 tensor(-0.9239, grad_fn=<AddBackward0>)\n",
      "54 tensor(-0.9218, grad_fn=<AddBackward0>)\n",
      "55 tensor(-0.9329, grad_fn=<AddBackward0>)\n",
      "56 tensor(-0.9372, grad_fn=<AddBackward0>)\n",
      "57 tensor(-0.9281, grad_fn=<AddBackward0>)\n",
      "58 tensor(-0.9337, grad_fn=<AddBackward0>)\n",
      "59 tensor(-0.9361, grad_fn=<AddBackward0>)\n",
      "60 tensor(-0.9259, grad_fn=<AddBackward0>)\n",
      "61 tensor(-0.9403, grad_fn=<AddBackward0>)\n",
      "62 tensor(-0.9296, grad_fn=<AddBackward0>)\n",
      "63 tensor(-0.9319, grad_fn=<AddBackward0>)\n",
      "64 tensor(-0.9388, grad_fn=<AddBackward0>)\n",
      "65 tensor(-0.9198, grad_fn=<AddBackward0>)\n",
      "66 tensor(-0.9346, grad_fn=<AddBackward0>)\n",
      "67 tensor(-0.9459, grad_fn=<AddBackward0>)\n",
      "68 tensor(-0.9400, grad_fn=<AddBackward0>)\n",
      "69 tensor(-0.9343, grad_fn=<AddBackward0>)\n",
      "70 tensor(-0.9400, grad_fn=<AddBackward0>)\n",
      "71 tensor(-0.9445, grad_fn=<AddBackward0>)\n",
      "72 tensor(-0.9460, grad_fn=<AddBackward0>)\n",
      "73 tensor(-0.9438, grad_fn=<AddBackward0>)\n",
      "74 tensor(-0.9400, grad_fn=<AddBackward0>)\n",
      "75 tensor(-0.9431, grad_fn=<AddBackward0>)\n",
      "76 tensor(-0.9397, grad_fn=<AddBackward0>)\n",
      "77 tensor(-0.9383, grad_fn=<AddBackward0>)\n",
      "78 tensor(-0.9407, grad_fn=<AddBackward0>)\n",
      "79 tensor(-0.9370, grad_fn=<AddBackward0>)\n",
      "80 tensor(-0.9379, grad_fn=<AddBackward0>)\n",
      "81 tensor(-0.9366, grad_fn=<AddBackward0>)\n",
      "82 tensor(-0.9491, grad_fn=<AddBackward0>)\n",
      "83 tensor(-0.9345, grad_fn=<AddBackward0>)\n",
      "84 tensor(-0.9439, grad_fn=<AddBackward0>)\n",
      "85 tensor(-0.9361, grad_fn=<AddBackward0>)\n",
      "86 tensor(-0.9427, grad_fn=<AddBackward0>)\n",
      "87 tensor(-0.9499, grad_fn=<AddBackward0>)\n",
      "88 tensor(-0.9499, grad_fn=<AddBackward0>)\n",
      "89 tensor(-0.9533, grad_fn=<AddBackward0>)\n",
      "90 tensor(-0.9566, grad_fn=<AddBackward0>)\n",
      "91 tensor(-0.9497, grad_fn=<AddBackward0>)\n",
      "92 tensor(-0.9597, grad_fn=<AddBackward0>)\n",
      "93 tensor(-0.9586, grad_fn=<AddBackward0>)\n",
      "94 tensor(-0.9618, grad_fn=<AddBackward0>)\n",
      "95 tensor(-0.9613, grad_fn=<AddBackward0>)\n",
      "96 tensor(-0.9621, grad_fn=<AddBackward0>)\n",
      "97 tensor(-0.9542, grad_fn=<AddBackward0>)\n",
      "98 tensor(-0.9529, grad_fn=<AddBackward0>)\n",
      "99 tensor(-0.9626, grad_fn=<AddBackward0>)\n",
      "100 tensor(-0.9632, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18()\n",
    "\n",
    "learner = SimSiam(model)\n",
    "\n",
    "opt = torch.optim.Adam(learner.parameters(), lr=0.001)\n",
    "\n",
    "criterion = NegativeCosineSimilarity()\n",
    "\n",
    "def sample_unlabelled_images():\n",
    "    return torch.randn(20, 3, 256, 256)\n",
    "\n",
    "for _ in range(100):\n",
    "    images1 = sample_unlabelled_images()\n",
    "    images2 = images1*0.9\n",
    "    p1, p2, z1, z2 = learner(images1, images2).values()\n",
    "    loss = criterion(p1, p2, z1, z2)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(_+1,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(x,y,z):\n",
    "    dic = {'x':x,\n",
    "           'y':y,\n",
    "           'z':z}\n",
    "    return dic\n",
    "x,y,z = test(1,2,3).values()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
